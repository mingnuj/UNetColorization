{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colorization_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpJZ0GGp3IVFw0QJiIZbj2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PN3tU9C-In3"
      },
      "source": [
        "# Get Dataset from Google Drive\n",
        "please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xIxIerf9-mv",
        "outputId": "c285ece7-badd-4794-a826-e537c1254dd8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CktQqzf9-KvQ"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzLfW9zC-R9h"
      },
      "source": [
        "# Color Hint Transform\n",
        "If you want to change how many hints you are giving, change the threshold values in call function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk-PfTI8-RLy"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ColorHintTransform(object):\n",
        "  def __init__(self, size=256, mode=\"training\"):\n",
        "    super(ColorHintTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "    self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "  def bgr_to_lab(self, img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, ab = lab[:, :, 0], lab[:, :, 1:]\n",
        "    return l, ab\n",
        "\n",
        "  def hint_mask(self, bgr, threshold=[0.95, 0.97, 0.99]):\n",
        "    h, w, c = bgr.shape\n",
        "    mask_threshold = random.choice(threshold)\n",
        "    mask = np.random.random([h, w, 1]) > mask_threshold\n",
        "    return mask\n",
        "\n",
        "  def img_to_mask(self, mask_img):\n",
        "    mask = mask_img[:, :, 0, np.newaxis] >= 255\n",
        "    return mask\n",
        "\n",
        "  def __call__(self, img, mask_img=None):\n",
        "    threshold = [0.95, 0.97, 0.99]\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      mask = self.hint_mask(image, threshold)\n",
        "\n",
        "      hint_image = image * mask\n",
        "\n",
        "      l, ab = self.bgr_to_lab(image)\n",
        "      l_hint, ab_hint = self.bgr_to_lab(hint_image)\n",
        "\n",
        "      return self.transform(l), self.transform(ab), self.transform(ab_hint)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      image = cv2.resize(img, (self.size, self.size))\n",
        "      hint_image = image * self.img_to_mask(mask_img)\n",
        "\n",
        "      l, _ = self.bgr_to_lab(image)\n",
        "      _, ab_hint = self.bgr_to_lab(hint_image)\n",
        "\n",
        "      return self.transform(l), self.transform(ab_hint)\n",
        "\n",
        "    else:\n",
        "      return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwIg8_c8-xuv"
      },
      "source": [
        "# Dataloader for Colorization Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hoFDrxa-w91"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class ColorHintDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(ColorHintDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "    self.hint = None\n",
        "    self.mask = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = ColorHintTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      hint_dir = os.path.join(self.root_path, \"hint\")\n",
        "      mask_dir = os.path.join(self.root_path, \"mask\")\n",
        "      self.hint = [os.path.join(self.root_path, \"hint\", dirs) for dirs in os.listdir(hint_dir)]\n",
        "      self.mask = [os.path.join(self.root_path, \"mask\", dirs) for dirs in os.listdir(mask_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.mode != \"testing\":\n",
        "      return len(self.examples)\n",
        "    else:\n",
        "      return len(self.hint)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode == \"testing\":\n",
        "      hint_file_name = self.hint[idx]\n",
        "      mask_file_name = self.mask[idx]\n",
        "      hint_img = cv2.imread(hint_file_name)\n",
        "      mask_img = cv2.imread(mask_file_name)\n",
        "\n",
        "      input_l, input_hint = self.transforms(hint_img, mask_img)\n",
        "      sample = {\"l\": input_l, \"hint\": input_hint,\n",
        "                \"file_name\": \"image_%06d.png\" % int(os.path.basename(hint_file_name).split('.')[0])}\n",
        "    else:\n",
        "      file_name = self.examples[idx]\n",
        "      img = cv2.imread(file_name)\n",
        "      l, ab, hint = self.transforms(img)\n",
        "      sample = {\"l\": l, \"ab\": ab, \"hint\": hint}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvFJzgkN-7rp"
      },
      "source": [
        "# Network\n",
        "Unet network for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnpz2JMF_LgP"
      },
      "source": [
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    # (convolution => [BN] => ReLU) * 2\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    # Downscaling with maxpool then double conv\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    # Upscaling then double conv\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldojqptO-269"
      },
      "source": [
        "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUeDcdun_cH2"
      },
      "source": [
        "# Tensorboard\n",
        "For training progress visualization. Run before training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NWpBPZy_g0o",
        "outputId": "5cd9db2e-8b56-4bcb-842a-a275901677ca"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 27.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.1.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fvd40J6_iGb"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weD_Kxlc_lZ3"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdJj8Ryg_mmI"
      },
      "source": [
        "# Training Phase\n",
        "Unet training code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOKtV2k3_pxx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data  as data\n",
        "\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "import torchvision.utils as tvutils\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from PIL import Image\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "save_path = \"/content/drive/MyDrive/Colorization_models\"\n",
        "\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "# Dataloader setting\n",
        "train_dataset = ColorHintDataset(root_path, 128)\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "val_dataset = ColorHintDataset(root_path, 128)\n",
        "val_dataset.set_mode(\"validation\")\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Model declaration\n",
        "net = UNet(2, 2)\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "# loss\n",
        "criterion = nn.MSELoss(size_average=False)\n",
        "\n",
        "if use_cuda:\n",
        "  model.to('cuda')\n",
        "  criterion.to('cuda')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, eps=1e-08)\n",
        "\n",
        "step = 0\n",
        "# Tensorboard writer\n",
        "writer = SummaryWriter(\"logs\")\n",
        "\n",
        "for epoch in range(30):\n",
        "  print('Epoch {}/{}'.format(epoch + 1, 30))\n",
        "  print('-' * 10)\n",
        "\n",
        "  for i, data in enumerate(tq.tqdm(train_dataloader)):\n",
        "    if use_cuda:\n",
        "      l = data[\"l\"].to('cuda')\n",
        "      ab = data[\"ab\"].to('cuda')\n",
        "      hint = data[\"hint\"].to('cuda')\n",
        "    \n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = model(hint)\n",
        "\n",
        "    loss = criterion(preds, ab)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    gt_image = torch.cat((l, ab), dim=1)\n",
        "    pred_image = torch.cat((l, preds), dim=1)\n",
        "    psnr_train = batch_PSNR(gt_image, pred_image, 1.)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      # Log the scalar values\n",
        "      writer.add_scalar('loss', loss.item(), step)\n",
        "      writer.add_scalar('PSNR on training data', psnr_train, step)\n",
        "\n",
        "      # log the images => Tensorboard\n",
        "      Img = tvutils.make_grid(gt_image.data, nrow=4, normalize=True, scale_each=True)\n",
        "      Irecon = tvutils.make_grid(pred_image.data, nrow=4, normalize=True, scale_each=True)\n",
        "      writer.add_image('GT image', Img, epoch)\n",
        "      writer.add_image('reconstructed image', Irecon, epoch)\n",
        "      print(\"[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\n",
        "          (epoch + 1, i + 1, len(train_dataloader), loss.item(), psnr_train))\n",
        "    step += 1\n",
        "    \n",
        "  torch.save(model.module.state_dict(), os.path.join(save_path, \"{}.tar\".format(epoch+1)))\n",
        "  print(\"saved at {}\".format(os.path.join(save_path, \"{}.tar\".format(epoch+1))))\n",
        "\n",
        "  psnr_val = []\n",
        "\n",
        "  # Validation on training phase\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for val_data in tq.tqdm(val_dataloader):\n",
        "      l = val_data[\"l\"].to('cuda')\n",
        "      ab = val_data[\"ab\"].to('cuda')\n",
        "      hint = val_data[\"hint\"].to('cuda')\n",
        "\n",
        "      preds = model(hint)\n",
        "\n",
        "      gt_image = torch.cat((l, ab), dim=1)\n",
        "      pred_image = torch.cat((l, preds), dim=1)\n",
        "      psnr = batch_PSNR(gt_image, pred_image, 1.)\n",
        "      psnr_val.append(psnr)\n",
        "\n",
        "      val_Img = tvutils.make_grid(gt_image.data, nrow=4, normalize=True, scale_each=True)\n",
        "      val_Irecon = tvutils.make_grid(pred_image.data, nrow=4, normalize=True, scale_each=True)\n",
        "      writer.add_image('validation gt image', val_Img, epoch)\n",
        "      writer.add_image('validation reconstructed image', val_Irecon, epoch+1)\n",
        "    \n",
        "    mean_val = np.mean(psnr_val)\n",
        "    print(\"\\n[epoch %d] PSNR_val: %.4f\" % (epoch + 1, mean_val))\n",
        "    writer.add_scalar('PSNR on validation data', mean_val, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5BjgdiCViB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data  as data\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from PIL import Image\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def image_save(img, path):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    img = np.asarray(transforms.ToPILImage()(img))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_LAB2BGR)\n",
        "  cv2.imwrite(path, img)\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "# Change to your data root directory\n",
        "image_path = \"/content/drive/MyDrive/Multimedia_test_dataset/colorization2/\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colorization_models/25.tar\"\n",
        "result_save_path = \"/content/drive/MyDrive/Multimedia_test_dataset/colorization_test_result\"\n",
        "\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "test_dataset = ColorHintDataset(image_path, 128)\n",
        "test_dataset.set_mode(\"testing\")\n",
        "\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "net = UNet(2, 2)\n",
        "\n",
        "if use_cuda:\n",
        "  net.to('cuda')\n",
        "\n",
        "net.load_state_dict(torch.load(checkpoint_path))\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(tq.tqdm(test_dataloader)):\n",
        "  if use_cuda:\n",
        "    l = data[\"l\"].to('cuda')\n",
        "    hint = data[\"hint\"].to('cuda')\n",
        "  file_name = data[\"file_name\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out_test = model(hint)\n",
        "    pred_image = torch.cat((l, out_test), dim=1)\n",
        "    for idx in range(len(file_name)):\n",
        "      image_save(pred_image[idx], os.path.join(result_save_path, file_name[idx]))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}